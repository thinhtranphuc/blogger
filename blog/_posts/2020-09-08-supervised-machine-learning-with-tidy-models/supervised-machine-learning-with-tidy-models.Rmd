---
title: "Supervised Machine Learning with Tidy Models"
description: |
  A short description of the post.
author: "Thinh Tran"
date: 09-08-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Acknowledgment 
Practice and Self-studying from the source of [Julia Sielge][https://supervised-ml-course.netlify.app]
# Loading packages 
```{r}
library(tidyverse)
theme_set(theme_light())
```

# Chapter 1: Mtcars
```{r}

cars2018 <- read_csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv")
```

The first step before modelling is to explore the data

## Exploratotry Data Analysis 
Quickly get the feels of how our data is 

```{r}
cars2018 %>% glimpse()
```

Let's ask some questions

### What is the distribution of `mpg`

```{r}
cars2018 %>% 
  ggplot(aes(mpg)) + 
  geom_histogram(bins=25) +
  scale_x_log10() + 
  labs(
    x = "Fuel efficiency(mpg)",
    y = "Number of cars"
  )
```

### Car models
```{r}
cars2018 %>%
  count(model, model_index, sort = TRUE)

cars2018 %>%
  count(aspiration, sort = TRUE)

cars2018 %>% 
  count(fuel_injection, sort = TRUE)
```


### Variation between models cars and fuel efficiency 

```{r}
cars2018 %>% 
  mutate(model_id = str_c(model, as.character(model_index), sep = ":"),
         model_id = fct_reorder(model_id, mpg)) %>% 
  arrange(desc(mpg)) %>% 
  head(10) %>% 
  ggplot(aes(model_id, mpg)) + 
  geom_col() + 
  coord_flip()


cars2018 %>% 
  mutate(model_id = str_c(model, as.character(model_index), sep = ":"),
         model_id = fct_reorder(model_id, mpg)) %>% 
  arrange(desc(mpg)) %>% 
  tail(10) %>% 
  ggplot(aes(model_id, mpg)) + 
  geom_col() + 
  coord_flip()

```

```{r}
cars2018 %>% 
  count(transmission, sort = TRUE) # quite unbalance --> handle with cares 
```


## Simple model 

```{r}
car_vars <- cars2018 %>% 
  select(-starts_with("model")) %>% 
  mutate(across(is.character, factor))
# Fit a linear model 

fit_all <- lm(mpg ~ ., data = car_vars)

summary(fit_all)
```


## Traning a model 

It's not a good choice to train a model on full datasets. Splitting the data into training dataset and testing dataset would help to identify the overfitting 

Let's use 

```{r}
library(tidymodels)
# splitting the data 

car_split <- initial_split(car_vars, 
                           prop = 0.8, strata = aspiration)

car_train <- training(car_split)
car_test <- testing(car_split)

## a linear regression model specification
lm_mod <- linear_reg() %>%
    set_engine("lm")

fit_lm <- lm_mod %>%
    fit(log(mpg) ~ ., 
        data = car_train)

fit_lm

## a random forest model specification
rf_mod <- rand_forest() %>%
    set_mode("regression") %>%
    set_engine("randomForest")

fit_rf <- rf_mod %>%
    fit(log(mpg) ~ ., 
        data = car_train)      

fit_rf
```


## Evaluate model performance 

What's we predict on training data
```{r}
# Create a new column
results <- car_train %>%
    mutate(mpg = log(mpg)) %>%
    bind_cols(predict(fit_lm, car_train) %>%
                  rename(.pred_lm = .pred)) %>%
    bind_cols(predict(fit_rf, car_train) %>%
                  rename(.pred_rf = .pred))


# Evaluate the result
metrics(results, truth = mpg, estimate = .pred_lm)
metrics(results, truth = mpg, estimate = .pred_rf)

```

Now apply for the testing dataset
```{r}
results <- car_test %>%
    mutate(mpg = log(mpg)) %>%
    bind_cols(predict(fit_lm, car_test) %>%
                  rename(.pred_lm = .pred)) %>%
    bind_cols(predict(fit_rf, car_test) %>%
                  rename(.pred_rf = .pred))


metrics(results, truth = mpg, estimate = .pred_lm)
metrics(results, truth = mpg, estimate = .pred_rf)
```


### Bootstrap resampling

```{r}
# create a bootstrap resamples
car_boot <- bootstraps(car_train)

# Evaluate the models with bootstrap resampling
lm_res <- lm_mod %>% 
  fit_resamples(
    log(mpg) ~ .,
    resamples = car_boot,
    control = control_resamples(save_pred = TRUE)
  )

rf_res <- rf_mod %>%
    fit_resamples(
        log(mpg) ~ .,
        resamples = car_boot,
        control = control_resamples(save_pred = TRUE)
    )
```


```{r}
results <-  bind_rows(lm_res %>%
                          collect_predictions() %>%
                          mutate(model = "lm"),
                      rf_res %>%
                          collect_predictions() %>%
                          mutate(model = "rf"))

glimpse(results)

results %>%
    ggplot(aes(`log(mpg)`, .pred)) +
    geom_abline(lty = 2, color = "gray50") +
    geom_point(aes(color = id), size = 1.5, alpha = 0.3, show.legend = FALSE) +
    geom_smooth(method = "lm") +
    facet_wrap(~ model)
```

